{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9rRHksOU8CSF"
      },
      "source": [
        "ICP 6"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JV-ddI007lsI"
      },
      "source": [
        "1. Save the model and use the saved model to predict on new text data (ex, “A lot of good things are happening. We are respected again throughout the world, and that's a great thing.@realDonaldTrump”) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y4L_fzJgzQtE",
        "outputId": "ae04044a-5a7a-44c4-c3ab-2b0f53b81945"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "291/291 - 48s - loss: 0.8208 - accuracy: 0.6428 - 48s/epoch - 166ms/step\n",
            "144/144 - 4s - loss: 0.7668 - accuracy: 0.6614 - 4s/epoch - 31ms/step\n",
            "0.7668231725692749\n",
            "0.6614242196083069\n",
            "['loss', 'accuracy']\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
        "from matplotlib import pyplot\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import re\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "data = pd.read_csv('/content/Sentiment (3).csv')\n",
        "# Keeping only the neccessary columns\n",
        "data = data[['text','sentiment']]\n",
        "\n",
        "data['text'] = data['text'].apply(lambda x: x.lower())\n",
        "data['text'] = data['text'].apply((lambda x: re.sub('[^a-zA-z0-9\\s]', '', x)))\n",
        "\n",
        "for idx, row in data.iterrows():\n",
        "    row[0] = row[0].replace('rt', ' ')\n",
        "\n",
        "max_fatures = 2000\n",
        "tokenizer = Tokenizer(num_words=max_fatures, split=' ')\n",
        "tokenizer.fit_on_texts(data['text'].values)\n",
        "X = tokenizer.texts_to_sequences(data['text'].values)\n",
        "\n",
        "X = pad_sequences(X)\n",
        "\n",
        "embed_dim = 128\n",
        "lstm_out = 196\n",
        "def createmodel():\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(max_fatures, embed_dim,input_length = X.shape[1]))\n",
        "    model.add(LSTM(lstm_out, dropout=0.2, recurrent_dropout=0.2))\n",
        "    model.add(Dense(3,activation='softmax'))\n",
        "    model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
        "    return model\n",
        "# print(model.summary())\n",
        "\n",
        "labelencoder = LabelEncoder()\n",
        "integer_encoded = labelencoder.fit_transform(data['sentiment'])\n",
        "y = to_categorical(integer_encoded)\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X,y, test_size = 0.33, random_state = 42)\n",
        "\n",
        "batch_size = 32\n",
        "model = createmodel()\n",
        "model.fit(X_train, Y_train, epochs = 1, batch_size=batch_size, verbose = 2)\n",
        "score,acc = model.evaluate(X_test,Y_test,verbose=2,batch_size=batch_size)\n",
        "print(score)\n",
        "print(acc)\n",
        "print(model.metrics_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F2MFm8GV10gL",
        "outputId": "64880d68-4616-4954-f9f2-cebbf3ed268f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ],
      "source": [
        "model.save(\"sentiment_model.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tVZ5Mccz1yS3",
        "outputId": "71b11630-dc7e-494c-e154-8c93c8faa1c5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 296ms/step\n",
            "Predicted Sentiment: Negative\n"
          ]
        }
      ],
      "source": [
        "import tweepy\n",
        "from keras.models import load_model\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import re\n",
        "\n",
        "# Load the saved model\n",
        "model = load_model(\"/content/sentiment_model.h5\")\n",
        "\n",
        "# Define a function for preprocessing text\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub('[^a-zA-z0-9\\s]', '', text)\n",
        "    return text\n",
        "\n",
        "# Example new text data\n",
        "new_text = \"A lot of good things are happening. We are respected again throughout the world, and that's a great thing. @realDonaldTrump\"\n",
        "\n",
        "# Preprocess the new text data\n",
        "new_text = preprocess_text(new_text)\n",
        "\n",
        "# Tokenize and pad the new text data\n",
        "max_fatures = 2000\n",
        "tokenizer = Tokenizer(num_words=max_fatures, split=' ')\n",
        "tokenizer.fit_on_texts([new_text])\n",
        "X_new = tokenizer.texts_to_sequences([new_text])\n",
        "X_new = pad_sequences(X_new, maxlen=model.input_shape[1])\n",
        "\n",
        "# Make predictions\n",
        "predictions = model.predict(X_new)\n",
        "\n",
        "# Determine the sentiment based on the prediction\n",
        "sentiments = ['Negative', 'Neutral', 'Positive']\n",
        "predicted_sentiment = sentiments[predictions.argmax()]\n",
        "\n",
        "# Print the result\n",
        "print(\"Predicted Sentiment: \" + predicted_sentiment)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "esNzaERf75Pt"
      },
      "source": [
        "**2. Apply GridSearchCV on the source code provided in the class**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BM8kmaZZ2wFs",
        "outputId": "10d20bab-a800-4e48-f1a9-3033c6d62b52"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting scikeras\n",
            "  Downloading scikeras-0.12.0-py3-none-any.whl (27 kB)\n",
            "Requirement already satisfied: packaging>=0.21 in /usr/local/lib/python3.10/dist-packages (from scikeras) (24.0)\n",
            "Requirement already satisfied: scikit-learn>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from scikeras) (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.0->scikeras) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.0->scikeras) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.0->scikeras) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.0->scikeras) (3.4.0)\n",
            "Installing collected packages: scikeras\n",
            "Successfully installed scikeras-0.12.0\n"
          ]
        }
      ],
      "source": [
        "!pip install scikeras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "GC--J1Py20s0"
      },
      "outputs": [],
      "source": [
        "from scikeras.wrappers import KerasClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XKfmgeRl24_-",
        "outputId": "20456679-4262-4f70-8864-a0b2cd9bbd64"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "194/194 - 37s - loss: 0.8596 - accuracy: 0.6328 - 37s/epoch - 192ms/step\n",
            "97/97 - 2s - 2s/epoch - 23ms/step\n",
            "194/194 - 41s - loss: 0.8563 - accuracy: 0.6297 - 41s/epoch - 210ms/step\n",
            "97/97 - 3s - 3s/epoch - 34ms/step\n",
            "194/194 - 36s - loss: 0.8773 - accuracy: 0.6278 - 36s/epoch - 186ms/step\n",
            "97/97 - 2s - 2s/epoch - 23ms/step\n",
            "194/194 - 32s - loss: 0.8712 - accuracy: 0.6326 - 32s/epoch - 167ms/step\n",
            "97/97 - 3s - 3s/epoch - 28ms/step\n",
            "194/194 - 33s - loss: 0.8588 - accuracy: 0.6292 - 33s/epoch - 171ms/step\n",
            "97/97 - 3s - 3s/epoch - 27ms/step\n",
            "194/194 - 34s - loss: 0.8675 - accuracy: 0.6252 - 34s/epoch - 173ms/step\n",
            "97/97 - 2s - 2s/epoch - 23ms/step\n",
            "Epoch 1/2\n",
            "194/194 - 33s - loss: 0.8632 - accuracy: 0.6300 - 33s/epoch - 171ms/step\n",
            "Epoch 2/2\n",
            "194/194 - 29s - loss: 0.7171 - accuracy: 0.6888 - 29s/epoch - 151ms/step\n",
            "97/97 - 3s - 3s/epoch - 32ms/step\n",
            "Epoch 1/2\n",
            "194/194 - 33s - loss: 0.8599 - accuracy: 0.6271 - 33s/epoch - 170ms/step\n",
            "Epoch 2/2\n",
            "194/194 - 30s - loss: 0.6978 - accuracy: 0.6991 - 30s/epoch - 157ms/step\n",
            "97/97 - 2s - 2s/epoch - 23ms/step\n",
            "Epoch 1/2\n",
            "194/194 - 35s - loss: 0.8553 - accuracy: 0.6285 - 35s/epoch - 179ms/step\n",
            "Epoch 2/2\n",
            "194/194 - 29s - loss: 0.6883 - accuracy: 0.7022 - 29s/epoch - 151ms/step\n",
            "97/97 - 2s - 2s/epoch - 23ms/step\n",
            "Epoch 1/2\n",
            "194/194 - 35s - loss: 0.8565 - accuracy: 0.6320 - 35s/epoch - 178ms/step\n",
            "Epoch 2/2\n",
            "194/194 - 29s - loss: 0.7122 - accuracy: 0.6949 - 29s/epoch - 150ms/step\n",
            "97/97 - 3s - 3s/epoch - 34ms/step\n",
            "Epoch 1/2\n",
            "194/194 - 33s - loss: 0.8660 - accuracy: 0.6295 - 33s/epoch - 168ms/step\n",
            "Epoch 2/2\n",
            "194/194 - 30s - loss: 0.7025 - accuracy: 0.6999 - 30s/epoch - 157ms/step\n",
            "97/97 - 2s - 2s/epoch - 23ms/step\n",
            "Epoch 1/2\n",
            "194/194 - 35s - loss: 0.8494 - accuracy: 0.6320 - 35s/epoch - 181ms/step\n",
            "Epoch 2/2\n",
            "194/194 - 30s - loss: 0.6845 - accuracy: 0.7093 - 30s/epoch - 156ms/step\n",
            "97/97 - 3s - 3s/epoch - 30ms/step\n",
            "97/97 - 30s - loss: 0.8820 - accuracy: 0.6182 - 30s/epoch - 309ms/step\n",
            "49/49 - 3s - 3s/epoch - 51ms/step\n",
            "97/97 - 28s - loss: 0.8731 - accuracy: 0.6228 - 28s/epoch - 290ms/step\n",
            "49/49 - 3s - 3s/epoch - 52ms/step\n",
            "97/97 - 30s - loss: 0.8955 - accuracy: 0.6165 - 30s/epoch - 307ms/step\n",
            "49/49 - 2s - 2s/epoch - 51ms/step\n",
            "97/97 - 29s - loss: 0.8696 - accuracy: 0.6263 - 29s/epoch - 298ms/step\n",
            "49/49 - 2s - 2s/epoch - 50ms/step\n",
            "97/97 - 29s - loss: 0.8740 - accuracy: 0.6218 - 29s/epoch - 304ms/step\n",
            "49/49 - 3s - 3s/epoch - 65ms/step\n",
            "97/97 - 28s - loss: 0.8783 - accuracy: 0.6241 - 28s/epoch - 289ms/step\n",
            "49/49 - 3s - 3s/epoch - 67ms/step\n",
            "Epoch 1/2\n",
            "97/97 - 29s - loss: 0.8779 - accuracy: 0.6242 - 29s/epoch - 302ms/step\n",
            "Epoch 2/2\n",
            "97/97 - 25s - loss: 0.7220 - accuracy: 0.6949 - 25s/epoch - 259ms/step\n",
            "49/49 - 3s - 3s/epoch - 68ms/step\n",
            "Epoch 1/2\n",
            "97/97 - 29s - loss: 0.8862 - accuracy: 0.6176 - 29s/epoch - 303ms/step\n",
            "Epoch 2/2\n",
            "97/97 - 25s - loss: 0.7242 - accuracy: 0.6894 - 25s/epoch - 254ms/step\n",
            "49/49 - 2s - 2s/epoch - 50ms/step\n",
            "Epoch 1/2\n",
            "97/97 - 28s - loss: 0.8839 - accuracy: 0.6164 - 28s/epoch - 287ms/step\n",
            "Epoch 2/2\n",
            "97/97 - 25s - loss: 0.7149 - accuracy: 0.6877 - 25s/epoch - 255ms/step\n",
            "49/49 - 3s - 3s/epoch - 52ms/step\n",
            "Epoch 1/2\n",
            "97/97 - 30s - loss: 0.8833 - accuracy: 0.6216 - 30s/epoch - 309ms/step\n",
            "Epoch 2/2\n",
            "97/97 - 26s - loss: 0.7304 - accuracy: 0.6931 - 26s/epoch - 272ms/step\n",
            "49/49 - 4s - 4s/epoch - 83ms/step\n",
            "Epoch 1/2\n",
            "97/97 - 39s - loss: 0.8786 - accuracy: 0.6179 - 39s/epoch - 398ms/step\n",
            "Epoch 2/2\n",
            "97/97 - 27s - loss: 0.7233 - accuracy: 0.6889 - 27s/epoch - 278ms/step\n",
            "49/49 - 4s - 4s/epoch - 83ms/step\n",
            "Epoch 1/2\n",
            "97/97 - 33s - loss: 0.8707 - accuracy: 0.6198 - 33s/epoch - 336ms/step\n",
            "Epoch 2/2\n",
            "97/97 - 30s - loss: 0.7207 - accuracy: 0.6833 - 30s/epoch - 308ms/step\n",
            "49/49 - 3s - 3s/epoch - 52ms/step\n",
            "Epoch 1/2\n",
            "291/291 - 49s - loss: 0.8301 - accuracy: 0.6416 - 49s/epoch - 170ms/step\n",
            "Epoch 2/2\n",
            "291/291 - 46s - loss: 0.6884 - accuracy: 0.7066 - 46s/epoch - 158ms/step\n",
            "Best: 0.672548 using {'batch_size': 32, 'epochs': 2, 'optimizer': 'adam'}\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from scikeras.wrappers import KerasClassifier\n",
        "\n",
        "# Assuming the data loading and preprocessing steps are the same\n",
        "\n",
        "max_features = 2000\n",
        "tokenizer = Tokenizer(num_words=max_features, split=' ')\n",
        "# Assuming tokenizer fitting and text preprocessing is done here\n",
        "\n",
        "def createmodel(optimizer='adam'):\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(max_features, embed_dim, input_length=X.shape[1]))\n",
        "    model.add(SpatialDropout1D(0.2))\n",
        "    model.add(LSTM(lstm_out, dropout=0.2, recurrent_dropout=0.2))\n",
        "    model.add(Dense(3, activation='softmax'))\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Define the KerasClassifier with the build_fn as our model creation function\n",
        "model = KerasClassifier(model=createmodel, verbose=2)\n",
        "\n",
        "# Define hyperparameters to tune\n",
        "param_grid = {\n",
        "    'batch_size': [32, 64],\n",
        "    'epochs': [1, 2],\n",
        "    'optimizer': ['adam', 'rmsprop']\n",
        "}\n",
        "\n",
        "# Initialize GridSearchCV\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=1, cv=3)\n",
        "# Fit GridSearchCV\n",
        "grid_result = grid.fit(X_train, Y_train)\n",
        "\n",
        "# Summarize results\n",
        "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
